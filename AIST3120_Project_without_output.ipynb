{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1582683,
     "status": "ok",
     "timestamp": 1744481096493,
     "user": {
      "displayName": "serene fung",
      "userId": "03574321217431434914"
     },
     "user_tz": -480
    },
    "id": "bIk_VZpsqW_3",
    "outputId": "6ee703a6-28c6-4b06-e2ca-bf9fe92f969e"
   },
   "outputs": [],
   "source": [
    "!pip install evaluate datasets transformers seqeval\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "label_list = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "checkpoint = \"microsoft/deberta-base\"\n",
    "num_labels = len(label_list)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/deberta-base\",\n",
    "    use_fast=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    aligned_labels = []\n",
    "    for i, labels in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        for word_id in word_ids:\n",
    "            # -100 is used to ignore subword tokens in the loss\n",
    "            label_ids.append(labels[word_id] if word_id is not None else -100)\n",
    "        aligned_labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# 4. Load model\n",
    "model = AutoModelForTokenClassification.from_pretrained(checkpoint, num_labels=num_labels)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "# 5. Define metrics\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# 6. Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./my_deberta_ner\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    # Instead of evaluation_strategy=\"epoch\", pick a step-based approach:\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=50\n",
    ")\n",
    "\n",
    "\n",
    "# 7. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 8. Train\n",
    "trainer.train()\n",
    "\n",
    "# 9. Evaluate\n",
    "metrics = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(\"Test Metrics:\", metrics)\n",
    "print(\"F1 Score:\", metrics[\"eval_f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ysFwucnszYZ5",
    "outputId": "bfadb4d7-02e3-4c57-a478-4a22aee0fa87"
   },
   "outputs": [],
   "source": [
    "!pip install evaluate datasets transformers seqeval pytorch-crf\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "from transformers import DebertaPreTrainedModel, DebertaModel\n",
    "\n",
    "class DebertaCRFForTokenClassification(DebertaPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Custom DeBERTa model with a CRF layer on top for token classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.deberta = DebertaModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.crf = CRF(self.num_labels, batch_first=True)\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        labels=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # 1. Extract embeddings from DeBERTa\n",
    "        deberta_kwargs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "        }\n",
    "\n",
    "        outputs = self.deberta(**{k: v for k, v in deberta_kwargs.items() if v is not None})\n",
    "\n",
    "\n",
    "        # 2. Classifier + dropout\n",
    "        sequence_output = self.dropout(outputs[0])  # (batch_size, seq_len, hidden_dim)\n",
    "        logits = self.classifier(sequence_output)   # (batch_size, seq_len, num_labels)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # build a mask of valid token positions (labels != -100)\n",
    "            mask = labels != -100\n",
    "\n",
    "            # clamp -100 labels to zero so CRF doesn't see them as invalid indexes\n",
    "            labels_clamped = labels.clone()\n",
    "            labels_clamped[labels_clamped == -100] = 0\n",
    "\n",
    "            log_likelihood = self.crf(logits, tags=labels_clamped, mask=mask)\n",
    "            loss = -1 * log_likelihood\n",
    "\n",
    "        output = (logits,)\n",
    "        return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "# -------------------------\n",
    "# 1. Load the custom model code\n",
    "# -------------------------\n",
    "\n",
    "# 2. Load dataset\n",
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "label_list = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "num_labels = len(label_list)\n",
    "\n",
    "checkpoint = \"microsoft/deberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    checkpoint,\n",
    "    use_fast=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    aligned_labels = []\n",
    "    for i, labels in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(labels[word_id])\n",
    "\n",
    "        # Ensure first token is not masked for CRF\n",
    "        if label_ids[0] == -100:\n",
    "            for j, l in enumerate(label_ids):\n",
    "                if l != -100:\n",
    "                    label_ids[0] = l  # move first real label up\n",
    "                    break\n",
    "\n",
    "        aligned_labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels  # this line should only appear once, after the loop\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# 3. Create the CRF model\n",
    "model = DebertaCRFForTokenClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "# 4. Load the seqeval metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "\n",
    "    # ---------------------\n",
    "    # CRF decode step\n",
    "    # ---------------------\n",
    "    # 1. Convert raw logits to tensor\n",
    "    device = model.device\n",
    "    logits_tensor = torch.tensor(logits, dtype=torch.float32).to(device)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "    mask = labels_tensor != -100  # This will also be on the right device now\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if logits_tensor.ndim == 3:  # (batch_size, seq_len, num_labels)\n",
    "            predictions_list = model.crf.decode(logits_tensor, mask=mask)\n",
    "        else:  # already decoded predictions\n",
    "            predictions_list = logits_tensor.tolist()\n",
    "\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for preds, golds, m in zip(predictions_list, labels_tensor, mask):\n",
    "        preds_idx = 0\n",
    "        tmp_pred = []\n",
    "        tmp_gold = []\n",
    "        for gold_label, mask_val in zip(golds, m):\n",
    "            if mask_val.item() == 1:\n",
    "                # decode the predicted label ID\n",
    "                pred_id = preds[preds_idx]\n",
    "                preds_idx += 1\n",
    "                tmp_pred.append(label_list[pred_id])\n",
    "                tmp_gold.append(label_list[gold_label.item()])\n",
    "        true_predictions.append(tmp_pred)\n",
    "        true_labels.append(tmp_gold)\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# 5. Training Arguments (with AdamW, LR=2e-5, weight_decay=0.01, etc.)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./my_deberta_ner\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    num_train_epochs=3,   # or 5 if you're doing data augmentation\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,    # CRF config typically pairs well with some weight decay\n",
    "    logging_steps=50\n",
    ")\n",
    "\n",
    "# 6. Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 7. Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "executionInfo": {
     "elapsed": 52116,
     "status": "ok",
     "timestamp": 1744479059845,
     "user": {
      "displayName": "serene fung",
      "userId": "03574321217431434914"
     },
     "user_tz": -480
    },
    "id": "HK7OUPOE8-QJ",
    "outputId": "9c225728-b9e4-4ac8-d68c-5d67367123e7"
   },
   "outputs": [],
   "source": [
    "# 8. Evaluate on Test Set\n",
    "metrics = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(\"Test Metrics:\", metrics)\n",
    "print(\"F1 Score:\", metrics[\"eval_f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PRRea-8tRFp-"
   },
   "outputs": [],
   "source": [
    "!pip install nltk evaluate datasets transformers seqeval pytorch-crf\n",
    "\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "raw_datasets = load_dataset(\"conll2003\")\n",
    "label_list = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "checkpoint = \"microsoft/deberta-base\"\n",
    "num_labels = len(label_list)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/deberta-base\",\n",
    "    use_fast=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "\n",
    "# --------------------- Augmentation Functions ---------------------\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            name = lemma.name().replace('_', ' ')\n",
    "            if name.lower() != word.lower():\n",
    "                synonyms.add(name)\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(example, max_replacements=2):\n",
    "    tokens = example[\"tokens\"][:]\n",
    "    labels = example[\"ner_tags\"][:]\n",
    "    non_entity_indices = [i for i, label in enumerate(labels) if label == 0]\n",
    "    random.shuffle(non_entity_indices)\n",
    "    replaced = 0\n",
    "    for idx in non_entity_indices:\n",
    "        synonyms = get_synonyms(tokens[idx])\n",
    "        if synonyms:\n",
    "            tokens[idx] = random.choice(synonyms)\n",
    "            replaced += 1\n",
    "        if replaced >= max_replacements:\n",
    "            break\n",
    "    return {\"tokens\": tokens, \"ner_tags\": labels}\n",
    "\n",
    "def random_masking(example, max_masks=2):\n",
    "    tokens = example[\"tokens\"][:]\n",
    "    labels = example[\"ner_tags\"][:]\n",
    "    non_entity_indices = [i for i, label in enumerate(labels) if label == 0]\n",
    "    random.shuffle(non_entity_indices)\n",
    "    masked = 0\n",
    "    for idx in non_entity_indices[:max_masks]:\n",
    "        tokens[idx] = \"[MASK]\"\n",
    "        masked += 1\n",
    "        if masked >= max_masks:\n",
    "            break\n",
    "    return {\"tokens\": tokens, \"ner_tags\": labels}\n",
    "\n",
    "# --------------------- Apply Augmentation ---------------------\n",
    "train_data = raw_datasets[\"train\"].shuffle(seed=42)\n",
    "syn_replace_count = int(0.15 * len(train_data))\n",
    "mask_count = int(0.10 * len(train_data))\n",
    "\n",
    "syn_replace_set = train_data.select(range(syn_replace_count))\n",
    "mask_set = train_data.select(range(syn_replace_count, syn_replace_count + mask_count))\n",
    "rest_set = train_data.select(range(syn_replace_count + mask_count, len(train_data)))\n",
    "\n",
    "aug_syn = syn_replace_set.map(synonym_replacement)\n",
    "aug_mask = mask_set.map(random_masking)\n",
    "\n",
    "augmented_train = concatenate_datasets([aug_syn, aug_mask, rest_set]).shuffle(seed=42)\n",
    "\n",
    "# --------------------- Tokenizer & Alignment ---------------------\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    aligned_labels = []\n",
    "    for i, labels in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(labels[word_id])\n",
    "        if label_ids[0] == -100:\n",
    "            for j, l in enumerate(label_ids):\n",
    "                if l != -100:\n",
    "                    label_ids[0] = l\n",
    "                    break\n",
    "        aligned_labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_train = augmented_train.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=augmented_train.column_names\n",
    ")\n",
    "\n",
    "tokenized_val = raw_datasets[\"validation\"].map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names\n",
    ")\n",
    "\n",
    "# 5. Define metrics\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(checkpoint, num_labels=num_labels)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "# 6. Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./my_deberta_ner\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    # Instead of evaluation_strategy=\"epoch\", pick a step-based approach:\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=50\n",
    ")\n",
    "\n",
    "\n",
    "# 7. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 8. Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Ge3kJunSgVE"
   },
   "outputs": [],
   "source": [
    "# 9. Evaluate\n",
    "metrics = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(\"Test Metrics:\", metrics)\n",
    "print(\"F1 Score:\", metrics[\"eval_f1\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "",
   "provenance": [
    {
     "file_id": "1vNK2GfuR6Nsa5YKIJgRXok-IN1ds3rXe",
     "timestamp": 1744467446766
    }
   ],
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
